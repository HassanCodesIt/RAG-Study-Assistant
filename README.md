

# ğŸ“˜ PDF-Based Retrieval-Augmented Generation (RAG) System

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline that lets users upload PDFs (classified by subject), store their text embeddings in a **vector database**, and later **ask contextual questions** to retrieve precise, referenced answers from the document itself.

Built with **FastAPI**, **Groq LLM**, **HuggingFace Embeddings**, and **ChromaDB**, this system functions as a mini intelligent document Q&A platform.

---

## ğŸŒŸ Key Features

âœ… **Smart PDF Uploading** â€“ Upload PDFs categorized by subject.  
âœ… **Automatic Text Extraction** â€“ Uses `pdfminer` to extract readable text.  
âœ… **Text Chunking** â€“ Splits documents intelligently for better vectorization.  
âœ… **Semantic Embeddings** â€“ Encodes document chunks using HuggingFaceâ€™s `all-MiniLM-L6-v2`.  
âœ… **Vector Database** â€“ Stores and retrieves embeddings using `ChromaDB`.  
âœ… **Contextual LLM Querying** â€“ Uses `Groq`â€™s LLM API to generate elaborate, referenced answers.  
âœ… **FastAPI Backend** â€“ Manages upload and query routes efficiently.  
âœ… **Frontend Interface** â€“ Clean, responsive HTML pages for Uploading and Querying.  
âœ… **Persistent Storage** â€“ Uses `localStorage` to remember subjects locally.  
âœ… **Progress Bars & Spinners** â€“ Real-time user feedback during uploads.

---

## ğŸ§  Tech Stack

| Category | Technology | Purpose |
|-----------|-------------|----------|
| **Framework** | [FastAPI](https://fastapi.tiangolo.com/) | Backend API |
| **Language** | Python 3.10+ | Core application logic |
| **Embeddings Model** | [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) | Vector representation |
| **Vector Database** | [ChromaDB](https://www.trychroma.com/) | Document store and retrieval |
| **PDF Processing** | [pdfminer.six](https://pypi.org/project/pdfminer.six/) | Text extraction from PDFs |
| **LLM API** | [Groq API](https://groq.com/) | Natural language question-answering |
| **Frontend** | HTML, CSS, Vanilla JS | User Interface |
| **Text Splitter** | [LangChain RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/) | Efficient chunking for long texts |

---

## âš™ï¸ Architecture Overview

### ğŸ§© Workflow Summary

1. **Upload Phase**
   - User uploads a PDF â†’ FastAPI saves it â†’ Text extracted using `pdfminer`.
   - Text is split into semantic chunks (1000 characters with 150 overlap).
   - Each chunk is embedded using HuggingFace model â†’ Stored in ChromaDB.

2. **Query Phase**
   - User selects a subject and asks a question.
   - Query is embedded â†’ Compared to stored vectors.
   - Top `k` similar chunks retrieved.
   - Chunks are passed as â€œcontextâ€ to Groq LLM â†’ Generates final answer.

---

## ğŸ§­ System Architecture (Mermaid Diagram)

```mermaid
flowchart TD
A[User Uploads PDF] -->|POST /upload| B[FastAPI Backend]
B --> C[pdfminer: Extract Text]
C --> D[LangChain Splitter: Chunk Text]
D --> E[HuggingFace Embeddings]
E --> F[ChromaDB: Store Embeddings]

subgraph Database Layer
F[ChromaDB Vector Store]
end

G[User Asks Question] -->|POST /query| B
B --> H[HuggingFace Embedding for Query]
H --> I[ChromaDB: Similarity Search]
I --> J[Top-k Relevant Chunks]
J --> K[Groq LLM: Generate Answer]
K --> L[Response Sent to Frontend]
L --> M[Frontend Displays Answer]
````

---

## ğŸ” Query Sequence (Mermaid Sequence Diagram)

```mermaid
sequenceDiagram
    participant User
    participant Frontend
    participant FastAPI
    participant ChromaDB
    participant GroqLLM

    User->>Frontend: Select Subject + Type Question
    Frontend->>FastAPI: POST /query {subject, user_query}
    FastAPI->>ChromaDB: Query Vector Similarity
    ChromaDB-->>FastAPI: Return Top-k Relevant Chunks
    FastAPI->>GroqLLM: Send Context + Question
    GroqLLM-->>FastAPI: Return Generated Answer
    FastAPI-->>Frontend: JSON Response with Answer
    Frontend-->>User: Display Detailed Answer
```

---

## ğŸ§© Directory Structure

```
ğŸ“¦ project-root/
â”œâ”€â”€ main.py                    # FastAPI backend app
â”œâ”€â”€ subjects.py                # Core logic: PDF extraction, embedding, querying
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html             # Upload interface
â”‚   â””â”€â”€ query.html             # Query interface
â”œâ”€â”€ savepdf/                   # Saved PDFs
â”œâ”€â”€ vecDB1/                    # ChromaDB persistent store
â”œâ”€â”€ requirements.txt           # Dependencies
â””â”€â”€ README.md                  # Project documentation
```

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/yourusername/pdf-rag-system.git
cd pdf-rag-system
```

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate      # Windows
source venv/bin/activate   # Linux/Mac
```

### 3ï¸âƒ£ Install Requirements

```bash
pip install -r requirements.txt
```

#### requirements.txt

```text
fastapi
uvicorn
pdfminer.six
chromadb
langchain-text-splitters
langchain-community
sentence-transformers
groq
```

### 4ï¸âƒ£ Set API Key

In `subjects.py`, replace:

```python
client = Groq(api_key="api_key")
```

with your actual Groq API key:

```python
client = Groq(api_key="YOUR_GROQ_API_KEY")
```

### 5ï¸âƒ£ Run the Server

```bash
uvicorn main:app --reload
```

Open your browser and visit:
ğŸ‘‰ [http://127.0.0.1:8000](http://127.0.0.1:8000)

---

## ğŸŒ API Endpoints

| Method | Endpoint  | Description                                    |
| ------ | --------- | ---------------------------------------------- |
| `GET`  | `/`       | Returns Upload Page                            |
| `POST` | `/upload` | Uploads PDF, extracts text, creates embeddings |
| `GET`  | `/query`  | Returns Query Page                             |
| `POST` | `/query`  | Queries the LLM using Chroma context           |

---

## ğŸ§  Example Usage

**Step 1:** Upload

* Go to `/`
* Choose subject (e.g., *Chemistry*)
* Upload `organic_chemistry.pdf`

**Step 2:** Query

* Go to `/query`
* Choose *Chemistry*
* Ask: â€œExplain SN1 reaction mechanismâ€

**Step 3:** Output
LLM responds with:

```
The SN1 reaction mechanism proceeds via the formation of a carbocation...
(Mentioned in Paragraph 4 of the document)
```

---

## ğŸ’» Frontend Overview

### Upload Page

* Add new or choose existing subject.
* Upload PDF with real-time progress.
* Success and error handling via inline messages.

### Query Page

* Select a subject.
* Enter a natural language query.
* Displays formatted answer in a response box.

---

## ğŸ§© Key Functions Explained

### `extraction(file_path)`

Extracts text content from PDF using `pdfminer.six`.

### `vectordbadd(text, subject)`

* Splits text into chunks.
* Embeds chunks using `HuggingFaceEmbeddings`.
* Stores embeddings + text into ChromaDB.

### `vectordbget(subject, query)`

* Embeds query.
* Retrieves top-k relevant document chunks.

### `llm(prompt, context)`

* Sends question and retrieved context to Groq model.
* Returns an elaborate, referenced, markdown-free answer.

---

## ğŸ§° Local Storage Functionality

Frontend uses `localStorage` to persist subjects:

* New subjects added dynamically.
* Survive page refreshes.
* Makes switching between topics seamless.

---

## âš¡ Performance Considerations

* Efficient chunk size (1000 chars, 150 overlap).
* Fast retrieval via ChromaDB vector search.
* Lightweight transformer model ideal for CPU inference.
* Streaming response handling for low latency.

---

## ğŸ§± Future Enhancements

* [ ] Multi-file management per subject
* [ ] Embedding caching for faster reloads
* [ ] Authentication & user profiles
* [ ] Integration with local models (DeepSeek, Ollama)
* [ ] UI improvements with chat-like experience

---

## ğŸ“Š Architecture Diagram (Rendered via Mermaid)

(See interactive diagram above in FigJam preview)

---

## ğŸ§¾ License

MIT License Â© 2025 [Your Name]
You are free to use, modify, and distribute this project.

---

## ğŸ™ Acknowledgments

* [LangChain](https://www.langchain.com/)
* [HuggingFace](https://huggingface.co/)
* [Groq](https://groq.com/)
* [ChromaDB](https://www.trychroma.com/)
* [FastAPI](https://fastapi.tiangolo.com/)
* [pdfminer.six](https://pypi.org/project/pdfminer.six/)

---

### ğŸ Summary

This project is a **complete end-to-end RAG (Retrieval-Augmented Generation)** application â€” combining LLM intelligence with semantic retrieval and a clean user interface.
It showcases practical integration of **document parsing**, **vector search**, and **language model reasoning** in a modular and extensible architecture.

---

âœ¨ *Developed with Python, FastAPI, Groq LLM, and a lot of â˜• caffeine.*






```
