---

````markdown
# ğŸ“š RAG Study Assistant â€“ Where PDFs Meet AI Intelligence ğŸ¤–

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-0.110%2B-009688?logo=fastapi)
![VectorDB](https://img.shields.io/badge/ChromaDB-Vector%20Storage-8A2BE2)
![LLM](https://img.shields.io/badge/Groq-LLM-orange)
![Embeddings](https://img.shields.io/badge/HuggingFace-Embeddings-yellow)
![License](https://img.shields.io/badge/License-MIT-green)

A **Retrieval-Augmented Generation (RAG)** powered study assistant that transforms your PDFs into **interactive AI knowledge bases**.  
Upload course materials, textbooks, or research papers, and ask natural language questions â€” the assistant retrieves and explains answers **directly from your document context**.

---

## ğŸŒ Overview

The **RAG Study Assistant** bridges the gap between **traditional PDFs** and **modern AI learning**.  
It extracts, embeds, and intelligently queries PDF content â€” turning static files into searchable, context-aware knowledge systems.

ğŸ§  **Powered by:**  
- **FastAPI** for the web backend  
- **Groq LLM** for reasoning and contextual question-answering  
- **ChromaDB** for vector similarity search  
- **HuggingFace Transformers** for high-quality text embeddings  

---

## âš¡ Core Highlights

âœ… **Smart PDF Uploading** â€“ Categorize files by subject (Physics, Chemistry, etc.)  
âœ… **Text Extraction** â€“ Extracts readable text using `pdfminer.six`  
âœ… **Semantic Chunking** â€“ Breaks documents into manageable, meaningful parts  
âœ… **Vector Storage** â€“ Embeds and stores chunks using `ChromaDB`  
âœ… **Contextual Question Answering** â€“ Powered by `Groq LLM`  
âœ… **FastAPI Backend** â€“ Secure and scalable API handling  
âœ… **Minimal Frontend** â€“ Clean HTML/CSS UI for upload and query pages  
âœ… **Persistent Local Storage** â€“ Saves your subjects in browser localStorage  
âœ… **User Feedback UI** â€“ Real-time progress bar and success alerts  

---

## ğŸ§  Tech Stack

| Layer | Technology | Role |
|-------|-------------|------|
| **Framework** | [FastAPI](https://fastapi.tiangolo.com/) | Backend web API |
| **Language** | Python 3.10+ | Core logic |
| **Embeddings** | [Sentence-Transformers (all-MiniLM-L6-v2)](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) | Converts text â†’ vector |
| **Vector DB** | [ChromaDB](https://www.trychroma.com/) | Stores document embeddings |
| **LLM API** | [Groq](https://groq.com/) | Generates answers using document context |
| **PDF Parser** | [pdfminer.six](https://pypi.org/project/pdfminer.six/) | Extracts text from PDFs |
| **Frontend** | HTML, CSS, JS | Lightweight UI |
| **Splitter** | [LangChain RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/) | Text segmentation |

---

## ğŸ§© System Workflow

### ğŸ“¥ Upload Flow
1. User uploads a PDF and selects a subject.
2. FastAPI extracts text via `pdfminer.six`.
3. Text is split into chunks (1000 chars + 150 overlap).
4. Embeddings generated via `HuggingFace`.
5. Chunks + embeddings stored in ChromaDB by subject.

### ğŸ’¬ Query Flow
1. User selects a subject and enters a question.  
2. Query embedded â†’ top `k` relevant chunks fetched.  
3. Context + question sent to Groq LLM.  
4. Detailed, referenced answer returned.

---

## ğŸ§­ Architecture Diagram

```mermaid
flowchart TD
A[ğŸ“¤ User Uploads PDF] -->|POST /upload| B[FastAPI Backend]
B --> C[pdfminer: Extract Text]
C --> D[LangChain Splitter: Chunk Text]
D --> E[HuggingFace Embeddings]
E --> F[ChromaDB: Store Embeddings]

subgraph Database Layer
F[ChromaDB Vector Store]
end

G[ğŸ’¬ User Asks Question] -->|POST /query| B
B --> H[Embed Query with HuggingFace]
H --> I[Search in ChromaDB]
I --> J[Retrieve Top-k Relevant Chunks]
J --> K[Groq LLM: Generate Answer]
K --> L[Return JSON Response]
L --> M[Frontend Displays AI Answer]
````

---

## ğŸ” Sequence of Operations

```mermaid
sequenceDiagram
    participant User
    participant Frontend
    participant FastAPI
    participant ChromaDB
    participant GroqLLM

    User->>Frontend: Upload PDF / Ask Question
    Frontend->>FastAPI: POST /upload or /query
    FastAPI->>ChromaDB: Store or Retrieve Vectors
    ChromaDB-->>FastAPI: Return Document Chunks
    FastAPI->>GroqLLM: Send Context + Question
    GroqLLM-->>FastAPI: Return Detailed Answer
    FastAPI-->>Frontend: JSON Response
    Frontend-->>User: Display in Browser UI
```

---

## ğŸ“ Project Structure

```
ğŸ“¦ RAG-Study-Assistant/
â”œâ”€â”€ main.py                 # FastAPI application entrypoint
â”œâ”€â”€ subjects.py             # Core RAG logic (extract, embed, query)
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html          # Upload page
â”‚   â””â”€â”€ query.html          # Query page
â”œâ”€â”€ savepdf/                # Uploaded PDFs
â”œâ”€â”€ vecDB1/                 # Persistent Chroma vector DB
â”œâ”€â”€ requirements.txt        # Dependencies
â””â”€â”€ README.md               # Documentation
```

---

## ğŸ› ï¸ Installation Guide

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/HassanCodesIt/RAG-Study-Assistant.git
cd RAG-Study-Assistant
```

### 2ï¸âƒ£ Create a Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate     # Windows
source venv/bin/activate  # macOS/Linux
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Configure API Key

In `subjects.py`:

```python
client = Groq(api_key="YOUR_GROQ_API_KEY")
```

### 5ï¸âƒ£ Launch Application

```bash
uvicorn main:app --reload
```

Access it at ğŸ‘‰ **[http://127.0.0.1:8000](http://127.0.0.1:8000)**

---

## ğŸ§© Example Use Case

1. Upload: `Thermodynamics.pdf` under *Physics*
2. Query: â€œExplain the second law of thermodynamicsâ€
3. Response:

   ```
   The second law states that the total entropy of an isolated system always increases...
   (Mentioned in Paragraph 3 of your document)
   ```

---

## ğŸ–¥ï¸ Frontend Preview

**Upload Page:**

* Choose or create a subject
* Upload PDF with real-time progress bar
* Auto-saves new subjects locally

**Query Page:**

* Ask natural language questions
* Displays detailed AI answers with reference lines

---

## ğŸš€ Key Functions Explained

### `extraction(file_path)`

Extracts and cleans text from PDF.

### `vectordbadd(text, subject)`

Splits, embeds, and stores chunks in ChromaDB.

### `vectordbget(subject, query)`

Retrieves semantically similar document chunks.

### `llm(prompt, context)`

Generates contextual, paragraph-referenced answers via Groq LLM.

---

## âš¡ Performance Optimizations

* Lightweight model (`all-MiniLM-L6-v2`) ensures CPU efficiency.
* Persistent ChromaDB enables quick reloads.
* Streamed Groq responses minimize latency.
* Simple UI ensures fast load times and usability.

---

## ğŸ”® Future Enhancements

* [ ] Multi-file per subject support
* [ ] PDF-level metadata and file tracking
* [ ] Authentication for multi-user access
* [ ] Support for local models (DeepSeek, Ollama)
* [ ] Chat-style conversation memory

---

## ğŸ§¾ License

**MIT License Â© 2025 [HassanCodesIt](https://github.com/HassanCodesIt)**
Feel free to fork, modify, and expand this project.

---

## ğŸ™Œ Acknowledgments

* [LangChain](https://www.langchain.com/)
* [ChromaDB](https://www.trychroma.com/)
* [HuggingFace Transformers](https://huggingface.co/)
* [Groq LLM](https://groq.com/)
* [FastAPI](https://fastapi.tiangolo.com/)
* [pdfminer.six](https://pypi.org/project/pdfminer.six/)

---

> ğŸ§© *Built with FastAPI, Groq, HuggingFace, and caffeine â˜• â€” making PDFs talk intelligently.*

```

---

### ğŸ”§ Notes for You
If youâ€™d like this README to:
- Include a **banner image (top header)** with your project name
- Add a **â€œDemoâ€ section** with screenshots of your frontend  
- Or have **color-coded section headers (using HTML)**  

I can generate that version too â€” perfectly optimized for GitHubâ€™s dark mode (like the â€œAyurveda Chat Assistantâ€ style you showed).  

Would you like me to make that **final premium README version with header banner and color-tuned section design**?
```
